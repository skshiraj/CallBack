{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQDRNrY2NCXf"
   },
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\nsuguru\\\\Desktop\\\\data.csv\")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.loc[:,'f1':'f2']\n",
    "Y=data.loc[:,'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13400, 2)\n",
      "(6600, 2)\n",
      "(13400,)\n",
      "(6600,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\n",
    "        print('train_begin')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print('\\n','epoch_end')\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['acc'].append(logs.get('acc'))\n",
    "        #self.history['auc'].append(logs.get('auc'))\n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(logs.get('val_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13400 samples, validate on 6600 samples\n",
      "train_begin\n",
      "Epoch 1/25\n",
      " 1000/13400 [=>............................] - ETA: 1s - loss: 7.5746 - acc: 0.5060\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 23us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 2/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4980 - acc: 0.5110\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 3/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.8660 - acc: 0.4870\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 4/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.5900 - acc: 0.5050\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 5/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.8966 - acc: 0.4850\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 6/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.3446 - acc: 0.5210\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 7/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.7586 - acc: 0.4940\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 8/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.6360 - acc: 0.5020\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 9/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.9580 - acc: 0.4810\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 10/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4673 - acc: 0.5130\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 11/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4826 - acc: 0.5120\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 12/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4826 - acc: 0.5120\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 13/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.0533 - acc: 0.5400\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 14/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.6973 - acc: 0.4980\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 15/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4520 - acc: 0.5140\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 16/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4520 - acc: 0.5140\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 17/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.8200 - acc: 0.4900\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 18/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.7126 - acc: 0.4970\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 19/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.8966 - acc: 0.4850\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 20/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.9733 - acc: 0.4800\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 21/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4826 - acc: 0.5120\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 22/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.4060 - acc: 0.5170\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 23/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.3600 - acc: 0.5200\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 24/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.3446 - acc: 0.5210\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n",
      "Epoch 25/25\n",
      " 1000/13400 [=>............................] - ETA: 0s - loss: 7.7126 - acc: 0.4970\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 0s 2us/sample - loss: 7.6666 - acc: 0.5000 - val_loss: 7.6666 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2319a144f28>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "#Dense hidden layer\n",
    "layer1 = Dense(2,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1,seed=20))(input_layer)\n",
    "layer2 = Dense(2,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1,seed=30))(layer1)\n",
    "layer3 = Dense(2,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1,seed=40))(layer2)\n",
    "layer4 = Dense(2,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1,seed=50))(layer3)\n",
    "layer5 = Dense(2,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0,1,seed=60))(layer4)\n",
    "\n",
    "#output layer\n",
    "output = Dense(1,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=70))(layer5)\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "\n",
    "#Callbacks\n",
    "history_own = LossHistory()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=25, validation_data=(X_test,Y_test), batch_size=1000, callbacks=[history_own])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8613519075396123,\n",
       "  0.7646200962242351,\n",
       "  0.7632974074561316,\n",
       "  0.7593458646499732,\n",
       "  0.7615424006863205],\n",
       " 'acc': [0.49589553, 0.5159702, 0.5210448, 0.5185075, 0.5156717],\n",
       " 'val_loss': [0.7348190634539633,\n",
       "  0.708384757834402,\n",
       "  0.7773649608750235,\n",
       "  0.6992647547568336,\n",
       "  0.7293022869753115],\n",
       " 'val_acc': [0.5386364, 0.5386364, 0.5386364, 0.5, 0.5]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_own.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13400 samples, validate on 6600 samples\n",
      "train_begin\n",
      "13337/13400 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.5219\n",
      " epoch_end\n",
      "13400/13400 [==============================] - 13s 995us/sample - loss: 0.7577 - acc: 0.5225 - val_loss: 0.6999 - val_acc: 0.5386\n",
      "[[3.9818615e-01 6.0081124e-01 1.2541220e-04 ... 1.2544045e-04\n",
      "  1.2542572e-04 1.2543997e-04]\n",
      " [3.9818615e-01 6.0081124e-01 1.2541220e-04 ... 1.2544045e-04\n",
      "  1.2542572e-04 1.2543997e-04]\n",
      " [3.9818615e-01 6.0081124e-01 1.2541220e-04 ... 1.2544045e-04\n",
      "  1.2542572e-04 1.2543997e-04]\n",
      " ...\n",
      " [3.9818615e-01 6.0081124e-01 1.2541220e-04 ... 1.2544045e-04\n",
      "  1.2542572e-04 1.2543997e-04]\n",
      " [7.2471070e-01 2.6788750e-01 9.2581153e-04 ... 9.2602440e-04\n",
      "  9.2591927e-04 9.2600763e-04]\n",
      " [3.9818615e-01 6.0081124e-01 1.2541220e-04 ... 1.2544045e-04\n",
      "  1.2542572e-04 1.2543997e-04]]\n"
     ]
    }
   ],
   "source": [
    "history_own = LossHistory()\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,epochs=1, validation_data=(X_test,Y_test), batch_size=1, callbacks=[history_own])\n",
    "u=model.predict(X_train)\n",
    "print(u)\n",
    "#predict(x,batch_size=None,verbose=0,steps=None,callbacks=None,max_queue_size=10,workers=1,use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.9818615e-01, 6.0081124e-01, 1.2541220e-04, 1.2466905e-04,\n",
       "       1.2541508e-04, 1.2544426e-04, 1.2540504e-04, 1.2544045e-04,\n",
       "       1.2542572e-04, 1.2543997e-04], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Call_Backs_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
